import pandas as pd
import numpy as np
import pickle
import os
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from gensim.models import Word2Vec
import logging
import sys
import matplotlib.pyplot as plt
import seaborn as sns

# --- Configuration ---
BEHAVIORAL_FILE = (
    "/Workspace/Users/jwang77@optumcloud.com/gpd-persona-ai-model-api/"
    "data/s-learning-data/behavior/normalized_us_dce_pro_behavioral_features_0401_2025_0420_2025.csv"
)
PLAN_FILE = (
    "/Workspace/Users/jwang77@optumcloud.com/gpd-persona-ai-model-api/"
    "data/s-learning-data/training/plan_derivation_by_zip.csv"
)
MODEL_FILE = (
    "/Workspace/Users/jwang77@optumcloud.com/gpd-persona-ai-model-api/"
    "data/s-learning-data/models/model-persona-1.1.1.pkl"
)
LABEL_ENCODER_FILE = (
    "/Workspace/Users/jwang77@optumcloud.com/gpd-persona-ai-model-api/"
    "data/s-learning-data/models/label_encoder_1.pkl"
)
TRANSFORMER_FILE = (
    "/Workspace/Users/jwang77@optumcloud.com/gpd-persona-ai-model-api/"
    "data/s-learning-data/models/power_transformer.pkl"
)

PERSONAS = ['dental', 'doctor', 'dsnp', 'drug', 'csnp']

PERSONA_THRESHOLD = {
    'drug': 0.28,
    'dental': 0.25,
    'doctor': 0.22,
    'dsnp': 0.25,
    'csnp': 0.24
}

PERSONA_INFO = {
    'csnp': {'plan_col': 'csnp', 'query_col': 'query_csnp', 'filter_col': 'filter_csnp', 'time_col': 'time_csnp_pages', 'accordion_col': 'accordion_csnp'},
    'dental': {'plan_col': 'ma_dental_benefit', 'query_col': 'query_dental', 'filter_col': 'filter_dental', 'time_col': 'time_dental_pages', 'accordion_col': 'accordion_dental'},
    'doctor': {'plan_col': 'ma_provider_network', 'query_col': 'query_provider', 'filter_col': 'filter_provider', 'click_col': 'click_provider'},
    'dsnp': {'plan_col': 'dsnp', 'query_col': 'query_dsnp', 'filter_col': 'filter_dsnp', 'time_col': 'time_dsnp_pages', 'accordion_col': 'accordion_dsnp'},
    'drug': {'plan_col': 'ma_drug_benefit', 'query_col': 'query_drug', 'filter_col': 'filter_drug', 'click_col': 'click_drug', 'time_col': 'time_drug_pages', 'accordion_col': 'accordion_drug'}
}

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)],
    force=True
)
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# --- Helper Functions ---
def safe_bool_to_int(boolean_value, df):
    if isinstance(boolean_value, pd.Series):
        return boolean_value.astype(int)
    return pd.Series([int(boolean_value)] * len(df), index=df.index)

def get_feature_as_series(df, col_name, default=0):
    if col_name in df.columns:
        return df[col_name]
    return pd.Series([default] * len(df), index=df.index)

def normalize_persona(df):
    valid_personas = PERSONAS
    new_rows = []
    invalid_personas = set()
    
    for _, row in df.iterrows():
        persona = row['persona']
        if pd.isna(persona) or not persona:
            continue
        
        personas = [p.strip().lower() for p in str(persona).split(',')]
        valid_found = [p for p in personas if p in valid_personas]
        
        if not valid_found:
            invalid_personas.update(personas)
            continue
        
        row_copy = row.copy()
        row_copy['persona'] = valid_found[0]
        new_rows.append(row_copy)
    
    result = pd.DataFrame(new_rows).reset_index(drop=True)
    logger.info(f"Rows after persona normalization: {len(result)}")
    if invalid_personas:
        logger.info(f"Invalid personas found: {invalid_personas}")
    if result.empty:
        logger.warning(f"No valid personas found. Valid personas: {valid_personas}")
    return result

def calculate_persona_weight(row, persona_info, persona):
    # Used to generate input features (e.g., dental_weight), not post-prediction weights
    query_col = persona_info['query_col']
    filter_col = persona_info['filter_col']
    plan_col = persona_info.get('plan_col', None)
    click_col = persona_info.get('click_col', None)
    
    query_value = row.get(query_col, 0) if pd.notna(row.get(query_col, np.nan)) else 0
    filter_value = row.get(filter_col, 0) if pd.notna(row.get(filter_col, np.nan)) else 0
    plan_value = row.get(plan_col, 0) if plan_col and pd.notna(row.get(plan_col, np.nan)) else 0
    click_value = row.get(click_col, 0) if click_col and pd.notna(row.get(click_col, np.nan)) else 0
    
    max_val = max([query_value, filter_value, plan_value, click_value, 1])
    if max_val > 0:
        query_value /= max_val
        filter_value /= max_val
        plan_value /= max_val
        click_value /= max_val
    
    weight = 0.25 * (query_value + filter_value + plan_value + click_value)
    return min(max(weight, 0), 1.0)

def load_data(behavioral_path, plan_path):
    try:
        behavioral_df = pd.read_csv(behavioral_path)
        logger.info(f"Raw behavioral data rows: {len(behavioral_df)}")
        
        persona_mapping = {'fitness': 'otc', 'hearing': 'vision'}
        behavioral_df['persona'] = behavioral_df['persona'].replace(persona_mapping)
        behavioral_df['persona'] = behavioral_df['persona'].astype(str).str.lower().str.strip()
        
        behavioral_df['zip'] = behavioral_df['zip'].fillna('unknown')
        behavioral_df['plan_id'] = behavioral_df['plan_id'].fillna('unknown')
        if 'total_session_time' in behavioral_df.columns:
            behavioral_df['total_session_time'] = behavioral_df['total_session_time'].fillna(0)
        
        plan_df = pd.read_csv(plan_path)
        plan_df['zip'] = plan_df['zip'].astype(str).str.strip()
        plan_df['plan_id'] = plan_df['plan_id'].astype(str).str.strip()
        
        return behavioral_df, plan_df
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        raise

def prepare_features(behavioral_df, plan_df, expected_features=None):
    try:
        behavioral_df = normalize_persona(behavioral_df)
        
        if behavioral_df.empty:
            logger.warning("Behavioral_df is empty after normalization. Using plan_df only.")
            training_df = plan_df.copy()
            training_df['persona'] = 'dental'
        else:
            training_df = behavioral_df.merge(
                plan_df.rename(columns={'StateCode': 'state'}),
                how='left', on=['zip', 'plan_id']
            ).reset_index(drop=True)
            logger.info(f"Rows after merge: {len(training_df)}")
        
        plan_features = ['ma_dental_benefit', 'csnp', 'dsnp', 'ma_drug_benefit', 'ma_provider_network']
        for col in plan_features:
            training_df[col] = training_df.get(col, pd.Series([0] * len(training_df), index=training_df.index)).fillna(0)
        
        behavioral_features = [
            'query_dental', 'query_drug', 'query_provider', 'query_csnp', 'query_dsnp',
            'filter_dental', 'filter_drug', 'filter_provider', 'filter_csnp', 'filter_dsnp',
            'num_pages_viewed', 'total_session_time', 'time_dental_pages', 'num_clicks',
            'time_csnp_pages', 'time_drug_pages', 'time_dsnp_pages',
            'accordion_csnp', 'accordion_dental', 'accordion_drug', 'accordion_dsnp'
        ]
        
        sparse_features = ['query_dental', 'time_dental_pages', 'query_drug', 'query_provider']
        imputer_median = SimpleImputer(strategy='median')
        imputer_zero = SimpleImputer(strategy='constant', fill_value=0)
        
        for col in behavioral_features:
            if col in training_df.columns:
                if col in sparse_features:
                    training_df[col] = imputer_zero.fit_transform(training_df[[col]]).flatten()
                else:
                    training_df[col] = imputer_median.fit_transform(training_df[[col]]).flatten()
            else:
                training_df[col] = pd.Series([0] * len(training_df), index=training_df.index)
        
        # Remove the loop for ['dental', 'vision'] and only keep for 'dental'
        for persona in ['dental']:
            query_col = PERSONA_INFO[persona]['query_col']
            time_col = PERSONA_INFO[persona].get('time_col', None)
            if query_col in training_df.columns and time_col in training_df.columns:
                strong_signal = (training_df[query_col] > training_df[query_col].quantile(0.9)) | \
                               (training_df[time_col] > training_df[time_col].quantile(0.9))
                training_df.loc[strong_signal, query_col] *= 1.5
                training_df.loc[strong_signal, time_col] *= 1.5
        
        if 'start_time' in training_df.columns:
            try:
                start_time = pd.to_datetime(training_df['start_time'], errors='coerce')
                training_df['recency'] = (pd.to_datetime('2025-05-27') - start_time).dt.days.fillna(30)
                training_df['time_of_day'] = start_time.dt.hour.fillna(12) // 6
                training_df['visit_frequency'] = training_df.groupby('userid')['start_time'].transform('count').fillna(1) / 30 if 'userid' in training_df.columns else pd.Series([1] * len(training_df), index=training_df.index)
            except:
                training_df['recency'] = pd.Series([30] * len(training_df), index=training_df.index)
                training_df['time_of_day'] = pd.Series([2] * len(training_df), index=training_df.index)
                training_df['visit_frequency'] = pd.Series([1] * len(training_df), index=training_df.index)
        else:
            training_df['recency'] = pd.Series([30] * len(training_df), index=training_df.index)
            training_df['visit_frequency'] = pd.Series([1] * len(training_df), index=training_df.index)
            training_df['time_of_day'] = pd.Series([2] * len(training_df), index=training_df.index)
        
        cluster_features = ['num_pages_viewed', 'total_session_time', 'num_clicks']
        if all(col in training_df.columns for col in cluster_features):
            kmeans = KMeans(n_clusters=5, random_state=42)
            training_df['user_cluster'] = kmeans.fit_predict(training_df[cluster_features].fillna(0))
        else:
            training_df['user_cluster'] = pd.Series([0] * len(training_df), index=training_df.index)
        
        training_df['dental_time_ratio'] = training_df.get('time_dental_pages', 0) / (training_df.get('total_session_time', 1) + 1e-5)
        training_df['click_ratio'] = training_df.get('num_clicks', 0) / (training_df.get('num_pages_viewed', 1) + 1e-5)
        
        if 'plan_id' in training_df.columns:
            plan_sentences = training_df.groupby('userid')['plan_id'].apply(list).tolist()
            w2v_model = Word2Vec(sentences=plan_sentences, vector_size=10, window=5, min_count=1, workers=4)
            plan_embeddings = training_df['plan_id'].apply(
                lambda x: w2v_model.wv[x] if x in w2v_model.wv else np.zeros(10)
            )
            embedding_cols = [f'plan_emb_{i}' for i in range(10)]
            training_df[embedding_cols] = pd.DataFrame(plan_embeddings.tolist(), index=training_df.index)
        else:
            embedding_cols = [f'plan_emb_{i}' for i in range(10)]
            for col in embedding_cols:
                training_df[col] = pd.Series([0] * len(training_df), index=training_df.index)
        
        query_cols = [c for c in behavioral_features if c.startswith('query_') and c in training_df.columns]
        filter_cols = [c for c in behavioral_features if c.startswith('filter_') and c in training_df.columns]
        training_df['query_count'] = training_df[query_cols].sum(axis=1) if query_cols else pd.Series([0] * len(training_df), index=training_df.index)
        training_df['filter_count'] = training_df[filter_cols].sum(axis=1) if filter_cols else pd.Series([0] * len(training_df), index=training_df.index)
        
        # Generate weight features required by the trained model
        for persona in PERSONAS:
            if persona in PERSONA_INFO:
                training_df[f'{persona}_weight'] = training_df.apply(
                    lambda row: calculate_persona_weight(row, PERSONA_INFO[persona], persona), axis=1
                )
        
        additional_features = []
        for persona in PERSONAS:
            persona_info = PERSONA_INFO.get(persona, {})
            query_col = get_feature_as_series(training_df, persona_info.get('query_col'))
            filter_col = get_feature_as_series(training_df, persona_info.get('filter_col'))
            click_col = get_feature_as_series(training_df, persona_info.get('click_col', 'dummy_col'))
            time_col = get_feature_as_series(training_df, persona_info.get('time_col', 'dummy_col'))
            accordion_col = get_feature_as_series(training_df, persona_info.get('accordion_col', 'dummy_col'))
            plan_col = get_feature_as_series(training_df, persona_info.get('plan_col'))
            
            signal_weights = 3.5 if persona == 'drug' else 3.0
            training_df[f'{persona}_signal'] = (
                query_col * 2.0 +
                filter_col * 2.0 +
                time_col.clip(upper=5) * 1.5 +
                accordion_col * 1.0 +
                click_col * 2.0
            ) * signal_weights
            additional_features.append(f'{persona}_signal')
            
            has_interaction = ((query_col > 0) | (filter_col > 0) | (click_col > 0) | (accordion_col > 0))
            training_df[f'{persona}_interaction'] = safe_bool_to_int(has_interaction, training_df) * 3.0
            additional_features.append(f'{persona}_interaction')
            
            training_df[f'{persona}_primary'] = (
                safe_bool_to_int(query_col > 0, training_df) * 2.0 +
                safe_bool_to_int(filter_col > 0, training_df) * 2.0 +
                safe_bool_to_int(click_col > 0, training_df) * 2.0 +
                safe_bool_to_int(time_col > 2, training_df) * 1.5
            ) * 2.0
            additional_features.append(f'{persona}_primary')
            
            training_df[f'{persona}_plan_correlation'] = plan_col * (
                query_col + filter_col + click_col + time_col.clip(upper=3)
            ) * 2.0
            additional_features.append(f'{persona}_plan_correlation')
        
        dental_query = get_feature_as_series(training_df, 'query_dental')
        dental_filter = get_feature_as_series(training_df, 'filter_dental')
        dental_time = get_feature_as_series(training_df, 'time_dental_pages')
        dental_accordion = get_feature_as_series(training_df, 'accordion_dental')
        dental_benefit = get_feature_as_series(training_df, 'ma_dental_benefit')
        
        training_df['dental_engagement_score'] = (
            dental_query * 3.0 +
            dental_filter * 3.0 +
            dental_time.clip(upper=5) * 2.0 +
            dental_accordion * 2.0 +
            dental_benefit * 4.0
        ) * 3.0
        additional_features.append('dental_engagement_score')
        
        training_df['dental_benefit_multiplier'] = (
            (dental_query + dental_filter + dental_accordion) * 
            (dental_benefit + 0.5) * 5.0
        ).clip(lower=0, upper=20)
        additional_features.append('dental_benefit_multiplier')
        
        # Add the missing dental_specificity feature after dental_benefit_multiplier
        training_df['dental_specificity'] = (
            dental_query * 3.0 - 
            (training_df.get('query_drug', 0) + training_df.get('query_provider', 0)) * 0.7
        ).clip(lower=0) * 4.0
        additional_features.append('dental_specificity')
        
        # Add the missing dental_combined_signal feature
        training_df['dental_combined_signal'] = (
            dental_query * 2.5 +
            dental_filter * 2.5 +
            dental_time.clip(upper=5) * 2.0 +
            dental_accordion * 1.5 +
            dental_benefit * 3.0
        ) * 2.8
        additional_features.append('dental_combined_signal')
        
        provider_query = get_feature_as_series(training_df, 'query_provider')
        provider_filter = get_feature_as_series(training_df, 'filter_provider')
        provider_click = get_feature_as_series(training_df, 'click_provider')
        provider_network = get_feature_as_series(training_df, 'ma_provider_network')
        
        # Add the missing doctor_network_boost feature - required by the model
        training_df['doctor_network_boost'] = (
            (provider_query + provider_filter + provider_click) *
            (provider_network + 0.5) * 5.0
        ).clip(lower=0, upper=25)
        additional_features.append('doctor_network_boost')
        
        training_df['doctor_interaction_score'] = (
            provider_query * 3.0 +
            provider_filter * 3.0 +
            provider_click * 5.0 +
            provider_network * 4.0
        ) * 4.0
        additional_features.append('doctor_interaction_score')
        
        # Add missing doctor_query_specificity feature
        training_df['doctor_query_specificity'] = (
            provider_query * 4.0 - 
            (training_df.get('query_dental', 0) + training_df.get('query_drug', 0)) * 0.8
        ).clip(lower=0) * 3.5
        additional_features.append('doctor_query_specificity')
        
        training_df['doctor_specificity'] = (
            provider_query * 3.0 - 
            (training_df.get('query_dental', 0) + training_df.get('query_drug', 0)) * 0.7
        ).clip(lower=0) * 4.0
        additional_features.append('doctor_specificity')
        
        dsnp_query = get_feature_as_series(training_df, 'query_dsnp')
        dsnp_filter = get_feature_as_series(training_df, 'filter_dsnp')
        dsnp_time = get_feature_as_series(training_df, 'time_dsnp_pages')
        dsnp_accordion = get_feature_as_series(training_df, 'accordion_dsnp')
        dsnp_plan = get_feature_as_series(training_df, 'dsnp')
        csnp_query = get_feature_as_series(training_df, 'query_csnp')
        
        training_df['dsnp_csnp_ratio'] = (
            (dsnp_query + 0.8) / (csnp_query + dsnp_query + 1e-5)
        ).clip(0, 1) * 5.0
        additional_features.append('dsnp_csnp_ratio')
        
        training_df['dsnp_engagement_score'] = (
            dsnp_query * 3.0 +
            dsnp_filter * 3.0 +
            dsnp_time.clip(upper=5) * 2.0 +
            dsnp_accordion * 2.0 +
            dsnp_plan * 5.0
        ) * 3.0
        additional_features.append('dsnp_engagement_score')
        
        training_df['dsnp_plan_multiplier'] = (
            (dsnp_query + dsnp_filter + dsnp_accordion) *
            (dsnp_plan + 0.5) * 5.0
        ).clip(lower=0, upper=20)
        additional_features.append('dsnp_plan_multiplier')
        
        drug_query = get_feature_as_series(training_df, 'query_drug')
        drug_filter = get_feature_as_series(training_df, 'filter_drug')
        drug_time = get_feature_as_series(training_df, 'time_drug_pages')
        drug_accordion = get_feature_as_series(training_df, 'accordion_drug')
        drug_click = get_feature_as_series(training_df, 'click_drug')
        drug_benefit = get_feature_as_series(training_df, 'ma_drug_benefit')
        
        training_df['drug_engagement_score'] = (
            drug_query * 3.0 +
            drug_filter * 3.0 +
            drug_time.clip(upper=5) * 2.0 +
            drug_accordion * 2.0 +
            drug_click * 4.0 +
            drug_benefit * 4.0
        ) * 3.5
        additional_features.append('drug_engagement_score')
        
        training_df['drug_interest_ratio'] = (
            (drug_query + drug_filter) /
            (training_df.get('query_count', 1) + training_df.get('filter_count', 1) + 1e-5)
        ).clip(upper=0.9) * 10.0
        additional_features.append('drug_interest_ratio')
        
        training_df['drug_benefit_boost'] = (
            (drug_query + drug_filter + drug_click + drug_accordion) *
            (drug_benefit + 0.5) * 5.0
        ).clip(lower=0, upper=25)
        additional_features.append('drug_benefit_boost')
        
        csnp_query = get_feature_as_series(training_df, 'query_csnp')
        csnp_filter = get_feature_as_series(training_df, 'filter_csnp')
        csnp_time = get_feature_as_series(training_df, 'time_csnp_pages')
        csnp_accordion = get_feature_as_series(training_df, 'accordion_csnp')
        csnp_plan = get_feature_as_series(training_df, 'csnp')
        dsnp_query = get_feature_as_series(training_df, 'query_dsnp')
        
        training_df['csnp_dsnp_ratio'] = (
            (csnp_query + 0.8) / (dsnp_query + csnp_query + 1e-5)
        ).clip(0, 1) * 5.0
        additional_features.append('csnp_dsnp_ratio')
        
        training_df['csnp_specificity'] = (
            csnp_query * 3.0 - 
            (training_df.get('query_dental', 0) + 
             training_df.get('query_drug', 0)) * 0.8
        ).clip(lower=0) * 4.0
        additional_features.append('csnp_specificity')
        
        training_df['csnp_engagement_score'] = (
            csnp_query * 3.0 +
            csnp_filter * 3.0 +
            csnp_time.clip(upper=5) * 2.0 +
            csnp_accordion * 2.0 +
            csnp_plan * 5.0
        ) * 4.0
        additional_features.append('csnp_engagement_score')
        
        training_df['csnp_plan_multiplier'] = (
            (csnp_query + csnp_filter + csnp_accordion) *
            (csnp_plan + 0.5) * 6.0
        ).clip(lower=0, upper=24)
        additional_features.append('csnp_plan_multiplier')
        
        # Remove any vision features from feature_columns and expected_features logic
        feature_columns = [
            col for col in behavioral_features + plan_features + additional_features + [
                'recency', 'visit_frequency', 'time_of_day', 'user_cluster', 'dental_time_ratio', 'click_ratio'
            ] + embedding_cols + [f'{persona}_weight' for persona in PERSONAS if persona in PERSONA_INFO]
            if not (
                col.startswith('vision') or
                col.endswith('_vision') or
                col in ['query_vision', 'filter_vision', 'vision_signal', 'vision_interaction', 'vision_primary', 'vision_weight']
            )
        ]
        X = training_df[feature_columns].fillna(0)
        y = training_df['persona']
        if expected_features is not None:
            # Remove vision features from expected_features
            expected_features = [f for f in expected_features if not (
                f.startswith('vision') or
                f.endswith('_vision') or
                f in ['query_vision', 'filter_vision', 'vision_signal', 'vision_interaction', 'vision_primary', 'vision_weight']
            )]
            missing_features = [f for f in expected_features if f not in X.columns]
            if missing_features:
                logger.error(f"Missing features: {missing_features}")
                raise ValueError(f"Missing features: {missing_features}")
            extra_features = [f for f in X.columns if f not in expected_features]
            if extra_features:
                X = X[expected_features]
        
        return X, y
    except Exception as e:
        logger.error(f"Failed to prepare features: {e}")
        raise

def compute_per_persona_accuracy(y_true, y_pred, classes, class_names):
    per_persona_accuracy = {}
    for cls_idx, cls_name in enumerate(class_names):
        if cls_name not in PERSONAS:
            continue
        mask = y_true == cls_idx
        if mask.sum() > 0:
            cls_accuracy = accuracy_score(y_true[mask], y_pred[mask])
            per_persona_accuracy[cls_name] = cls_accuracy * 100
        else:
            per_persona_accuracy[cls_name] = 0.0
    return per_persona_accuracy

def custom_ensemble_with_balanced_focus(predictions, binary_probas, le, weights=None, thresholds=None):
    if weights is None:
        weights = {persona: 1.0 for persona in PERSONAS}
    if thresholds is None:
        thresholds = PERSONA_THRESHOLD
    
    weighted_preds = np.copy(predictions)
    
    for i, persona in enumerate(le.classes_):
        weighted_preds[:, i] *= weights.get(persona, 1.0)
    
    if binary_probas:
        for persona, proba in binary_probas.items():
            persona_idx = np.where(le.classes_ == persona)[0][0]
            blend_ratio = 0.35 if persona in ['doctor', 'csnp'] else \
                         0.4 if persona in ['drug', 'dsnp'] else 0.5
            weighted_preds[:, persona_idx] = blend_ratio * weighted_preds[:, persona_idx] + \
                                            (1 - blend_ratio) * proba
    
    row_sums = weighted_preds.sum(axis=1, keepdims=True)
    normalized_preds = weighted_preds / (row_sums + 1e-8)
    
    predictions = np.zeros(len(normalized_preds), dtype=np.int32)
    for i in range(len(normalized_preds)):
        max_prob = -1
        max_idx = -1
        for j, persona in enumerate(le.classes_):
            prob = normalized_preds[i, j]
            threshold = thresholds.get(persona, 0.3)
            if prob > threshold and prob > max_prob:
                max_prob = prob
                max_idx = j
        if max_idx >= 0:
            predictions[i] = max_idx
        else:
            predictions[i] = np.argmax(normalized_preds[i])
    
    return predictions, normalized_preds

def evaluate_model_with_params(main_model, binary_classifiers, le, transformer, X_test, y_test_encoded, X_test_cols):
    try:
        # Apply transformer to the test data
        X_test_transformed = transformer.transform(X_test)
        
        # Get multi-class probabilities
        y_pred_probas_multi = main_model.predict_proba(X_test_transformed)

        # Get binary probabilities
        binary_probas = {}
        for persona in le.classes_:
            if persona in binary_classifiers:
                binary_probas[persona] = binary_classifiers[persona].predict_proba(X_test_transformed)[:, 1]
            else:
                binary_probas[persona] = np.zeros(len(X_test))

        # Check for problematic predictions (all zeros or NaNs)
        if np.isnan(y_pred_probas_multi).any():
            logger.warning("NaN values found in multi-class predictions!")
            y_pred_probas_multi = np.nan_to_num(y_pred_probas_multi)
            
        if np.sum(y_pred_probas_multi) == 0:
            logger.warning("All zero probabilities in multi-class predictions!")

        # Enhanced blending with focus on problematic personas
        y_pred_probas_multi_blended = np.copy(y_pred_probas_multi)
        for i, persona in enumerate(le.classes_):
            if persona in binary_probas:
                if persona == 'dental':
                    # Give much more weight to binary classifiers for problematic personas
                    blend_ratio = 0.2
                elif persona in ['doctor', 'csnp']:
                    blend_ratio = 0.35
                elif persona in ['drug', 'dsnp']:
                    blend_ratio = 0.4
                else:
                    blend_ratio = 0.5
                    
                y_pred_probas_multi_blended[:, i] = blend_ratio * y_pred_probas_multi[:, i] + \
                                                  (1 - blend_ratio) * binary_probas[persona]
                
                # Additional boost for dental based on feature signals
                if persona == 'dental':
                    dental_signals = (X_test['dental_signal'] > X_test['dental_signal'].quantile(0.7)) | \
                                    (X_test['dental_engagement_score'] > X_test['dental_engagement_score'].quantile(0.7))
                    y_pred_probas_multi_blended[dental_signals, i] *= 1.5

        # Normalize probabilities
        row_sums = y_pred_probas_multi_blended.sum(axis=1, keepdims=True)
        y_pred_probas_multi_normalized = y_pred_probas_multi_blended / (row_sums + 1e-8)

        # Calculate predictions with adaptive thresholds
        y_pred = np.zeros(y_pred_probas_multi_normalized.shape[0], dtype=int)
        for i in range(len(y_pred_probas_multi_normalized)):
            # Use lower thresholds for dental to increase detection
            adjusted_thresholds = PERSONA_THRESHOLD.copy()
            adjusted_thresholds['dental'] = 0.15  # Much lower threshold to catch more dental signals
            
            # For test cases where dental features have strong signals but aren't being predicted
            dental_features = ['query_dental', 'filter_dental', 'time_dental_pages', 'dental_signal', 'dental_engagement_score']
            dental_score = sum([X_test.iloc[i][col] for col in dental_features if col in X_test.columns])
            if dental_score > 0:
                dental_idx = np.where(le.classes_ == 'dental')[0][0]
                y_pred_probas_multi_normalized[i, dental_idx] = max(y_pred_probas_multi_normalized[i, dental_idx], 
                                                                 dental_score / 20)  # Scale appropriately
            
            # Find best persona for this sample
            max_prob = -1
            max_idx = 0
            for j, persona in enumerate(le.classes_):
                prob = y_pred_probas_multi_normalized[i, j]
                threshold = adjusted_thresholds.get(persona, 0.3)
                if prob > threshold and prob > max_prob:
                    max_prob = prob
                    max_idx = j
            y_pred[i] = max_idx

        # Calculate metrics
        overall_acc = accuracy_score(y_test_encoded, y_pred)
        macro_f1 = f1_score(y_test_encoded, y_pred, average='macro')
        per_persona_acc = compute_per_persona_accuracy(y_test_encoded, y_pred, le.classes_, le.classes_)
        
        # Add diagnostic information
        class_distribution = pd.Series(y_test_encoded).value_counts().sort_index()
        prediction_distribution = pd.Series(y_pred).value_counts().sort_index()
        logger.info("\nClass distribution in test data:")
        for i, count in enumerate(class_distribution):
            if i < len(le.classes_):
                logger.info(f"{le.classes_[i]}: {count}")
        
        logger.info("\nClass distribution in predictions:")
        for i, count in enumerate(prediction_distribution):
            if i < len(le.classes_):
                logger.info(f"{le.classes_[i]}: {count}")
        
        # Feature importance analysis for problematic personas
        if hasattr(main_model, 'get_feature_importance'):
            logger.info("\nFeature importance for main model:")
            importance = main_model.get_feature_importance()
            features_importance = list(zip(X_test.columns, importance))
            for feature, imp in sorted(features_importance, key=lambda x: x[1], reverse=True)[:15]:
                logger.info(f"{feature}: {imp}")
        
        # Check for specific signals in the data
        for persona in ['dental']:
            signal_count = sum(1 for i in range(len(y_test_encoded)) if 
                             y_test_encoded[i] == le.transform([persona])[0])
            correct_predictions = sum(1 for i in range(len(y_pred)) if 
                                    y_pred[i] == le.transform([persona])[0] and 
                                    y_test_encoded[i] == le.transform([persona])[0])
            if signal_count > 0:
                logger.info(f"\n{persona} signal analysis:")
                logger.info(f"Total {persona} samples: {signal_count}")
                logger.info(f"Correctly predicted: {correct_predictions} ({correct_predictions/signal_count*100:.2f}%)")
                
                # For misclassified samples, show where they went
                misclassified = [le.classes_[y_pred[i]] for i in range(len(y_pred)) if 
                               y_test_encoded[i] == le.transform([persona])[0] and 
                               y_pred[i] != le.transform([persona])[0]]
                if misclassified:
                    misclassified_counts = pd.Series(misclassified).value_counts()
                    logger.info(f"{persona} misclassified as: {dict(misclassified_counts)}")

        return overall_acc, macro_f1, per_persona_acc, y_pred, y_pred_probas_multi_normalized
    except Exception as e:
        logger.error(f"Error in model evaluation: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise

def create_detailed_report(X_test, y_test, y_pred, y_proba, le):
    """Creates a detailed report for model diagnosis"""
    try:
        # Create a DataFrame with original features, true labels and predictions
        result_df = X_test.copy()
        result_df['true_persona'] = le.inverse_transform(y_test)
        result_df['predicted_persona'] = le.inverse_transform(y_pred)
        result_df['correct'] = result_df['true_persona'] == result_df['predicted_persona']
        
        # Add predicted probabilities
        for i, persona in enumerate(le.classes_):
            result_df[f'prob_{persona}'] = y_proba[:, i]
        
        # Identify problematic samples
        dental_samples = result_df[result_df['true_persona'] == 'dental']
        
        logger.info(f"\nDental samples analysis (total: {len(dental_samples)}):")
        if len(dental_samples) > 0:
            dental_correct = dental_samples[dental_samples['correct']]
            logger.info(f"Correctly classified: {len(dental_correct)} ({len(dental_correct)/len(dental_samples)*100:.2f}%)")
            
            # Feature distributions for dental samples
            dental_features = ['query_dental', 'filter_dental', 'time_dental_pages', 
                               'dental_signal', 'dental_engagement_score']
            logger.info("Average feature values for dental samples:")
            for feature in dental_features:
                if feature in dental_samples.columns:
                    avg_val = dental_samples[feature].mean()
                    logger.info(f"{feature}: {avg_val:.4f}")
        
        # Save detailed report for further analysis
        report_path = os.path.join(os.path.dirname(MODEL_FILE), "evaluation_detailed_report.csv")
        result_df.to_csv(report_path, index=False)
        logger.info(f"Detailed report saved to: {report_path}")
        
        return result_df
    except Exception as e:
        logger.error(f"Error creating detailed report: {str(e)}")
        return None

def create_visualizations(X_test, y_test, y_pred, le):
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix

    # Only keep indices for personas in PERSONAS
    persona_indices = [i for i, p in enumerate(le.classes_) if p in PERSONAS]
    filtered_classes = [le.classes_[i] for i in persona_indices]
    y_test_filtered = np.array([i for i in y_test if le.classes_[i] in PERSONAS])
    y_pred_filtered = np.array([i for i in y_pred if le.classes_[i] in PERSONAS])
    # Map to new indices
    y_test_filtered = np.array([filtered_classes.index(le.classes_[i]) for i in y_test if le.classes_[i] in PERSONAS])
    y_pred_filtered = np.array([filtered_classes.index(le.classes_[i]) for i in y_pred if le.classes_[i] in PERSONAS])

    # Confusion Matrix
    cm = confusion_matrix(y_test_filtered, y_pred_filtered, labels=range(len(filtered_classes)))
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=filtered_classes, yticklabels=filtered_classes)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.close()

    # Per-persona accuracy bar plot
    persona_acc = []
    for idx, persona in enumerate(filtered_classes):
        mask = y_test_filtered == idx
        acc = (y_pred_filtered[mask] == idx).mean() if mask.sum() > 0 else 0
        persona_acc.append(acc * 100)
    plt.figure(figsize=(8, 4))
    sns.barplot(x=list(filtered_classes), y=persona_acc)
    plt.ylabel('Accuracy (%)')
    plt.title('Per-Persona Accuracy')
    plt.tight_layout()
    plt.savefig('per_persona_accuracy.png')
    plt.close()
    logger.info('Saved confusion_matrix.png and per_persona_accuracy.png')

def main():
    logger.info("Starting evaluation...")
    
    try:
        if not os.path.exists(MODEL_FILE):
            raise FileNotFoundError(f"Model file not found: {MODEL_FILE}")
        with open(MODEL_FILE, 'rb') as f:
            main_model = pickle.load(f)
        if not os.path.exists(LABEL_ENCODER_FILE):
            raise FileNotFoundError(f"Label encoder file not found: {LABEL_ENCODER_FILE}")
        with open(LABEL_ENCODER_FILE, 'rb') as f:
            le = pickle.load(f)
        if not os.path.exists(TRANSFORMER_FILE):
            raise FileNotFoundError(f"Transformer file not found: {TRANSFORMER_FILE}")
        with open(TRANSFORMER_FILE, 'rb') as f:
            transformer = pickle.load(f)
    except Exception as e:
        logger.error(f"Failed to load model or files: {str(e)}")
        sys.exit(1)
    
    expected_features = getattr(main_model, 'feature_names_', None)
    if expected_features is None:
        logger.warning("Model does not provide feature_names_. Will use all available features.")
    
    try:
        behavioral_df, plan_df = load_data(BEHAVIORAL_FILE, PLAN_FILE)
        
        # Check for dental samples in the data
        if 'persona' in behavioral_df.columns:
            dental_samples = behavioral_df[behavioral_df['persona'].str.lower().str.contains('dental', na=False)]
            logger.info(f"Dental samples in raw data: {len(dental_samples)}")
        
        X_data, y_data = prepare_features(behavioral_df, plan_df, expected_features)
        y_data_encoded = le.transform(y_data)
        X_data_cols = X_data.columns

        # Ensure all features expected by the model are present in X_data
        if expected_features is not None:
            missing_cols = [col for col in expected_features if col not in X_data.columns]
            for col in missing_cols:
                X_data[col] = 0
            # Reorder columns to match model expectation
            X_data = X_data[expected_features]

        # Check class distribution
        class_distribution = pd.Series(y_data).value_counts()
        logger.info("\nClass distribution in evaluation data:")
        for persona, count in class_distribution.items():
            logger.info(f"{persona}: {count}")
        
    except Exception as e:
        logger.error(f"Failed to load and prepare data: {str(e)}")
        sys.exit(1)
    
    binary_classifiers = {}
    for persona in PERSONAS:
        binary_model_path = MODEL_FILE.replace('.pkl', f'_{persona}_binary.pkl')
        if os.path.exists(binary_model_path):
            with open(binary_model_path, 'rb') as f:
                binary_classifiers[persona] = pickle.load(f)
                logger.info(f"Loaded binary classifier for {persona}")
        else:
            logger.warning(f"Binary classifier not found for {persona}")
    
    overall_acc, macro_f1, per_persona_acc, y_pred, y_proba = evaluate_model_with_params(
        main_model, binary_classifiers, le, transformer, X_data, y_data_encoded, X_data_cols
    )
    
    logger.info(f"\nEvaluation Results:")
    logger.info(f"Overall Accuracy: {overall_acc*100:.2f}%")
    logger.info(f"Macro F1: {macro_f1:.2f}")
    
    logger.info("\nPer-Persona Accuracy:")
    for persona, acc in per_persona_acc.items():
        logger.info(f"{persona}: {acc:.2f}%")
    
    if overall_acc <= 0.8:
        logger.warning("Overall accuracy below 0.8. Retrain with adjusted PERSONA_OVERSAMPLING_RATIO or PERSONA_CLASS_WEIGHT.")
    
    # Create detailed report for further analysis
    detailed_report = create_detailed_report(X_data, y_data_encoded, y_pred, y_proba, le)
    
    # Create confusion matrix and other visualizations
    create_visualizations(X_data, y_data_encoded, y_pred, le)
    
    # Suggest fixes for the training script
    logger.info("\nRecommendations to improve training model:")
    logger.info("1. Increase oversampling ratio for dental:")
    logger.info("   PERSONA_OVERSAMPLING_RATIO = {'dental': 7.0, 'drug': 5.0, 'doctor': 5.5, 'dsnp': 4.5, 'csnp': 5.0}")
    logger.info("2. Adjust class weights to prioritize underperforming classes:")
    logger.info("   PERSONA_CLASS_WEIGHT = {'dental': 8.0, 'drug': 6.5, 'doctor': 7.0, 'dsnp': 6.0, 'csnp': 7.0}")
    logger.info("3. Lower prediction thresholds for dental:")
    logger.info("   PERSONA_THRESHOLD = {'dental': 0.18, 'drug': 0.25, 'doctor': 0.22, 'dsnp': 0.23, 'csnp': 0.22}")

    logger.info("\n===== TOTAL SUMMARY REPORT =====")
    logger.info(f"Total samples evaluated: {len(y_data)}")
    logger.info(f"Overall Accuracy: {overall_acc*100:.2f}%")
    logger.info(f"Macro F1 Score: {macro_f1:.4f}")
    logger.info("Per-Persona Accuracy:")
    for persona in PERSONAS:
        acc = per_persona_acc.get(persona, 0.0)
        logger.info(f"  {persona}: {acc:.2f}%")
    logger.info("================================\n")

if __name__ == "__main__":
    main()
